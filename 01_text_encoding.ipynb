{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "JF_yrvRiyaLi",
      "metadata": {
        "id": "JF_yrvRiyaLi"
      },
      "source": [
        "# üß†‚ú®**Text Encoding with Pretrained Language Models (BERT)**\n",
        "In this notebook, I demonstrate how to tokenize text and extract embeddings using a **pretrained BERT model** from Hugging Face‚Äôs `transformers` library."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NpYF_xDQ17n4",
      "metadata": {
        "id": "NpYF_xDQ17n4"
      },
      "source": [
        "## üéØ **Task Objective**\n",
        "\n",
        "- Understand how text is tokenized using a pretrained tokenizer.\n",
        "\n",
        "- Pass the tokens through a pretrained BERT model.\n",
        "\n",
        "- Extract the `[CLS]` token embedding as a representation of the sentence.\n",
        "\n",
        "- Compare the similarity of two sentences using cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DmFdgbwP2LcR",
      "metadata": {
        "id": "DmFdgbwP2LcR"
      },
      "source": [
        "## üì¶  **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f2964f2b",
      "metadata": {
        "id": "f2964f2b"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RqmMb_192KxR",
      "metadata": {
        "id": "RqmMb_192KxR"
      },
      "source": [
        " ## üß∞ **Loading the Pretrained BERT Model**\n",
        "\n",
        " In this step, I load the `bert-base-uncased` model and its tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ugDdI2HjvtLw",
      "metadata": {
        "collapsed": true,
        "id": "ugDdI2HjvtLw"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PfzPyL7DxAEY",
      "metadata": {
        "id": "PfzPyL7DxAEY"
      },
      "source": [
        "## üß™  **Tokenizing a Sentence and Extracting the Embedding**\n",
        "In this step, I tokenize a sentence using BERT and extract the `[CLS]` embedding. I also display:\n",
        "\n",
        "- The tokenized words\n",
        "\n",
        "- Their corresponding token IDs\n",
        "\n",
        "- A sample of the `[CLS]` embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "k8cQBksGvhdC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8cQBksGvhdC",
        "outputId": "c3933dba-b55e-4756-9cdb-ae142a0ca5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['[CLS]', 'text', 'encoding', 'with', 'bert', 'is', 'powerful', '.', '[SEP]']\n",
            "Token IDs: [101, 3793, 17181, 2007, 14324, 2003, 3928, 1012, 102]\n",
            "\n",
            "CLS embedding shape: torch.Size([1, 768])\n",
            "\n",
            "Sample CLS embedding (first 10 values):\n",
            "tensor([-0.5005, -0.1889, -0.1415, -0.0087, -0.5979, -0.5189, -0.0960,  0.1999,\n",
            "         0.0647, -0.4416])\n"
          ]
        }
      ],
      "source": [
        "text = \"Text encoding with BERT is powerful.\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"][0]\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", input_ids.tolist())\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "print(\"\\nCLS embedding shape:\", cls_embedding.shape)\n",
        "\n",
        "print(\"\\nSample CLS embedding (first 10 values):\")\n",
        "print(cls_embedding[0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NlxhqDMbwjcg",
      "metadata": {
        "id": "NlxhqDMbwjcg"
      },
      "source": [
        "## üìè **Comparing Multiple Sentences Using Cosine Similarity**\n",
        "In this step, I compare three sentences ‚Äî two similar, and one exactly the same as another. For each, I show token IDs, tokens, and cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dUenvMGlwBCy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUenvMGlwBCy",
        "outputId": "2898d388-d3d8-4cdc-c308-501bfd1c134b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity (text1 vs text2): 0.9456\n",
            "Cosine similarity (text1 vs text3): 1.0000 (should be ~1.0)\n",
            "\n",
            "Tokens and IDs:\n",
            "Text 1 Tokens : ['[CLS]', 'i', 'love', 'machine', 'learning', '.', '[SEP]']\n",
            "Text 1 IDs    : [101, 1045, 2293, 3698, 4083, 1012, 102]\n",
            "\n",
            "Text 2 Tokens : ['[CLS]', 'machine', 'learning', 'is', 'fascinating', '.', '[SEP]']\n",
            "Text 2 IDs    : [101, 3698, 4083, 2003, 17160, 1012, 102]\n",
            "\n",
            "Text 3 Tokens : ['[CLS]', 'i', 'love', 'machine', 'learning', '.', '[SEP]']\n",
            "Text 3 IDs    : [101, 1045, 2293, 3698, 4083, 1012, 102]\n",
            "\n",
            "Explanation:\n",
            "- text1 and text3 are identical, so their cosine similarity is 1.0 (or very close).\n",
            "- text1 and text2 share key phrases like 'machine learning', so they are semantically similar.\n",
            "- BERT's [CLS] embedding captures this semantic closeness even if the wording changes.\n"
          ]
        }
      ],
      "source": [
        "def get_cls_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return F.normalize(outputs.last_hidden_state[:, 0, :], p=2, dim=1), inputs[\"input_ids\"][0]\n",
        "\n",
        "# Input sentences\n",
        "text1 = \"I love machine learning.\"\n",
        "text2 = \"Machine learning is fascinating.\"\n",
        "text3 = \"I love machine learning.\"  # exactly the same as text1\n",
        "\n",
        "\n",
        "emb1, ids1 = get_cls_embedding(text1)\n",
        "emb2, ids2 = get_cls_embedding(text2)\n",
        "emb3, ids3 = get_cls_embedding(text3)\n",
        "\n",
        "tokens1 = tokenizer.convert_ids_to_tokens(ids1)\n",
        "tokens2 = tokenizer.convert_ids_to_tokens(ids2)\n",
        "tokens3 = tokenizer.convert_ids_to_tokens(ids3)\n",
        "\n",
        "\n",
        "sim12 = F.cosine_similarity(emb1, emb2)\n",
        "sim13 = F.cosine_similarity(emb1, emb3)\n",
        "\n",
        "print(f\"Cosine similarity (text1 vs text2): {sim12.item():.4f}\")\n",
        "print(f\"Cosine similarity (text1 vs text3): {sim13.item():.4f} (should be ~1.0)\")\n",
        "\n",
        "\n",
        "print(\"\\nTokens and IDs:\")\n",
        "print(\"Text 1 Tokens :\", tokens1)\n",
        "print(\"Text 1 IDs    :\", ids1.tolist())\n",
        "print(\"\\nText 2 Tokens :\", tokens2)\n",
        "print(\"Text 2 IDs    :\", ids2.tolist())\n",
        "print(\"\\nText 3 Tokens :\", tokens3)\n",
        "print(\"Text 3 IDs    :\", ids3.tolist())\n",
        "\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"- text1 and text3 are identical, so their cosine similarity is 1.0 (or very close).\")\n",
        "print(\"- text1 and text2 share key phrases like 'machine learning', so they are semantically similar.\")\n",
        "print(\"- BERT's [CLS] embedding captures this semantic closeness even if the wording changes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vNQcKgLz1fq6",
      "metadata": {
        "id": "vNQcKgLz1fq6"
      },
      "source": [
        "## üìù **Summary**\n",
        "In this task, I explored how pretrained BERT models convert text into meaningful numerical representations (machine-readable embeddings). Specifically, I:\n",
        "\n",
        "- **Tokenized raw text** into model-readable input using BERT‚Äôs tokenizer, revealing how words are broken down into subword tokens and their corresponding token IDs.\n",
        "\n",
        "- **Extracted the `[CLS]` token embedding**, which serves as a compact vector representation capturing the overall meaning of the sentence.\n",
        "\n",
        "- **Compared sentence embeddings** by computing cosine similarity, demonstrating how BERT captures semantic relationships ‚Äî identical or closely related sentences produce high similarity scores.\n",
        "\n",
        "This hands-on experience deepened my understanding of how transformer-based language models encode language, enabling numerous NLP applications such as text classification, semantic search, and clustering."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}