# ğŸ§ âœ¨ MultiEmbed: Unified Text, Image, and Multimodal Embeddings

**MultiEmbed** is a collection of notebooks demonstrating how to generate and compare feature embeddings from text, images, and combinations of both. This project showcases the power of pretrained models in transforming raw data into meaningful numerical representations for downstream applications like similarity search, classification, and retrieval.

## 1ï¸âƒ£ Text Encoding with BERT

ğŸ“„ **Module**: `text_encoding.ipynb`  
ğŸš€ **Model**: `bert-base-uncased` (Hugging Face Transformers)  
ğŸ“Œ **Highlights**:

- Tokenize text and extract [CLS] embeddings.
- Compare sentence similarity using cosine similarity.
- Understand how BERT captures semantic information in embeddings.

ğŸ“ˆ **Applications**: Semantic search, sentence classification, clustering.

## 2ï¸âƒ£ Image Encoding with ResNet-50

ğŸ–¼ï¸ **Module**: `image_encoding.ipynb`  
ğŸš€ **Model**: `resnet50` (TorchVision)  
ğŸ“Œ **Highlights**:
- Preprocess images and extract 2048-dimensional feature vectors.
- Normalize and compare image embeddings using cosine similarity.
- Demonstrates visual similarity detection with CNN features.

ğŸ“ˆ **Applications**: Image retrieval, clustering, object recognition.

## 3ï¸âƒ£ Multimodal Embeddings with CLIP

ğŸ–¼ï¸ + ğŸ“„ **Module**: `multimodal_encoding.ipynb`  
ğŸš€ **Model**: `openai/clip-vit-base-patch32` (Hugging Face)  
ğŸ“Œ **Highlights**:

- Encode images and corresponding text into a shared semantic space.
- Compare embeddings for semantic alignment.
- Includes an image captioning component using `vit-gpt2`.

ğŸ“ˆ **Applications**: Cross-modal retrieval, captioning, zero-shot classification.